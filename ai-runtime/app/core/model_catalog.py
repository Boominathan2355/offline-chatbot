"""
Model catalog with HuggingFace GGUF download URLs.
Contains popular models organized by size tier.
Each model has capability flags:
  - supportsVision: can process images
  - supportsThinking: can do chain-of-thought reasoning
  - supportsCode: specialized for code generation
"""

MODEL_CATALOG = [
    # ── Tiny (< 1 GB) ───────────────────────────────────────────────
    {
        "id": "llama-3.2-1b-instruct",
        "name": "Llama 3.2 1B Instruct",
        "filename": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "size": "0.8 GB",
        "sizeBytes": 800000000,
        "ram": "~2 GB",
        "params": "1B",
        "description": "Meta's smallest Llama. Blazing fast on any hardware.",
        "url": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "tier": "tiny",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "tinyllama-1.1b-chat",
        "name": "TinyLlama 1.1B Chat",
        "filename": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "size": "637 MB",
        "sizeBytes": 667000000,
        "ram": "~2 GB",
        "params": "1.1B",
        "description": "Ultra-lightweight model, great for testing and low-end hardware.",
        "url": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "tier": "tiny",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "qwen2.5-0.5b-instruct",
        "name": "Qwen 2.5 0.5B Instruct",
        "filename": "qwen2.5-0.5b-instruct-q4_k_m.gguf",
        "size": "0.4 GB",
        "sizeBytes": 400000000,
        "ram": "~1 GB",
        "params": "0.5B",
        "description": "Alibaba's tiniest model. Runs on virtually anything.",
        "url": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf",
        "tier": "tiny",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "qwen2.5-1.5b-instruct",
        "name": "Qwen 2.5 1.5B Instruct",
        "filename": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "size": "1.0 GB",
        "sizeBytes": 1000000000,
        "ram": "~2 GB",
        "params": "1.5B",
        "description": "Great balance of speed and quality for edge devices.",
        "url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "tier": "tiny",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "smollm2-1.7b-instruct",
        "name": "SmolLM2 1.7B Instruct",
        "filename": "SmolLM2-1.7B-Instruct-Q4_K_M.gguf",
        "size": "1.0 GB",
        "sizeBytes": 1050000000,
        "ram": "~2 GB",
        "params": "1.7B",
        "description": "HuggingFace's compact model. Surprisingly capable for its size.",
        "url": "https://huggingface.co/bartowski/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q4_K_M.gguf",
        "tier": "tiny",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },

    # ── Small (1–3 GB) ──────────────────────────────────────────────
    {
        "id": "stablelm-zephyr-3b",
        "name": "StableLM Zephyr 3B",
        "filename": "stablelm-zephyr-3b.Q4_K_M.gguf",
        "size": "1.8 GB",
        "sizeBytes": 1900000000,
        "ram": "~3 GB",
        "params": "3B",
        "description": "Stability AI's fast conversational model. Great quality for its size.",
        "url": "https://huggingface.co/TheBloke/stablelm-zephyr-3b-GGUF/resolve/main/stablelm-zephyr-3b.Q4_K_M.gguf",
        "tier": "small",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "phi-3-mini-4k",
        "name": "Phi-3 Mini 4K Instruct",
        "filename": "Phi-3-mini-4k-instruct-q4.gguf",
        "size": "2.2 GB",
        "sizeBytes": 2300000000,
        "ram": "~4 GB",
        "params": "3.8B",
        "description": "Microsoft's compact yet powerful model. Excellent reasoning for its size.",
        "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
        "tier": "small",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "phi-3.5-mini-instruct",
        "name": "Phi-3.5 Mini Instruct",
        "filename": "Phi-3.5-mini-instruct-Q4_K_M.gguf",
        "size": "2.2 GB",
        "sizeBytes": 2300000000,
        "ram": "~4 GB",
        "params": "3.8B",
        "description": "Upgraded Phi-3.5 with 128K context. Better long-document handling.",
        "url": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q4_K_M.gguf",
        "tier": "small",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "llama-3.2-3b-instruct",
        "name": "Llama 3.2 3B Instruct",
        "filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "size": "2.0 GB",
        "sizeBytes": 2100000000,
        "ram": "~4 GB",
        "params": "3B",
        "description": "Meta's latest small Llama model. Fast and capable.",
        "url": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "tier": "small",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "qwen2.5-3b-instruct",
        "name": "Qwen 2.5 3B Instruct",
        "filename": "Qwen2.5-3B-Instruct-Q4_K_M.gguf",
        "size": "2.0 GB",
        "sizeBytes": 2100000000,
        "ram": "~4 GB",
        "params": "3B",
        "description": "Alibaba's multilingual model. Strong in English and Chinese.",
        "url": "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf",
        "tier": "small",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "rocket-3b",
        "name": "Rocket 3B",
        "filename": "rocket-3b.Q4_K_M.gguf",
        "size": "1.8 GB",
        "sizeBytes": 1900000000,
        "ram": "~3 GB",
        "params": "3B",
        "description": "AgentLM fine-tuned for function calling and tool use.",
        "url": "https://huggingface.co/TheBloke/rocket-3B-GGUF/resolve/main/rocket-3b.Q4_K_M.gguf",
        "tier": "small",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "openchat-3.5-0106",
        "name": "OpenChat 3.5",
        "filename": "openchat-3.5-0106.Q4_K_M.gguf",
        "size": "4.4 GB",
        "sizeBytes": 4600000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Community fine-tune that punches well above its weight.",
        "url": "https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106.Q4_K_M.gguf",
        "tier": "small",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },

    # ── Medium (3–6 GB) ─────────────────────────────────────────────
    {
        "id": "mistral-7b-instruct",
        "name": "Mistral 7B Instruct v0.3",
        "filename": "mistral-7b-instruct-v0.3.Q4_K_M.gguf",
        "size": "4.4 GB",
        "sizeBytes": 4600000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Excellent all-rounder. Strong coding and reasoning abilities.",
        "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "llama-3-8b-instruct",
        "name": "Llama 3 8B Instruct",
        "filename": "Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
        "size": "4.9 GB",
        "sizeBytes": 5100000000,
        "ram": "~8 GB",
        "params": "8B",
        "description": "Meta's Llama 3. Top-tier performance in its class.",
        "url": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "llama-3.1-8b-instruct",
        "name": "Llama 3.1 8B Instruct",
        "filename": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "size": "4.9 GB",
        "sizeBytes": 5100000000,
        "ram": "~8 GB",
        "params": "8B",
        "description": "Updated Llama 3.1 with 128K context window and tool use.",
        "url": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "gemma-2-9b-instruct",
        "name": "Gemma 2 9B Instruct",
        "filename": "gemma-2-9b-it-Q4_K_M.gguf",
        "size": "5.8 GB",
        "sizeBytes": 6100000000,
        "ram": "~10 GB",
        "params": "9B",
        "description": "Google's Gemma 2. Strong multilingual and reasoning capabilities.",
        "url": "https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "qwen2.5-7b-instruct",
        "name": "Qwen 2.5 7B Instruct",
        "filename": "Qwen2.5-7B-Instruct-Q4_K_M.gguf",
        "size": "4.7 GB",
        "sizeBytes": 4900000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Alibaba's 7B model. Excellent coding and math abilities.",
        "url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "zephyr-7b-beta",
        "name": "Zephyr 7B Beta",
        "filename": "zephyr-7b-beta.Q4_K_M.gguf",
        "size": "4.4 GB",
        "sizeBytes": 4600000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "HuggingFace's alignment-tuned Mistral. Natural conversationalist.",
        "url": "https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "neural-chat-7b",
        "name": "Neural Chat 7B v3.1",
        "filename": "neural-chat-7b-v3-1.Q4_K_M.gguf",
        "size": "4.4 GB",
        "sizeBytes": 4600000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Intel's fine-tuned Mistral. Optimized for helpful dialogue.",
        "url": "https://huggingface.co/TheBloke/neural-chat-7B-v3-1-GGUF/resolve/main/neural-chat-7b-v3-1.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "deepseek-llm-7b-chat",
        "name": "DeepSeek LLM 7B Chat",
        "filename": "deepseek-llm-7b-chat.Q4_K_M.gguf",
        "size": "4.1 GB",
        "sizeBytes": 4300000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Original DeepSeek LLM. Strong general knowledge and reasoning.",
        "url": "https://huggingface.co/TheBloke/deepseek-llm-7b-chat-GGUF/resolve/main/deepseek-llm-7b-chat.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "starling-lm-7b-alpha",
        "name": "Starling LM 7B Alpha",
        "filename": "starling-lm-7b-alpha.Q4_K_M.gguf",
        "size": "4.4 GB",
        "sizeBytes": 4600000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "UC Berkeley's RLHF-tuned model. Excels at following instructions.",
        "url": "https://huggingface.co/TheBloke/Starling-LM-7B-alpha-GGUF/resolve/main/starling-lm-7b-alpha.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "dolphin-2.6-mistral-7b",
        "name": "Dolphin 2.6 Mistral 7B",
        "filename": "dolphin-2.6-mistral-7b.Q4_K_M.gguf",
        "size": "4.4 GB",
        "sizeBytes": 4600000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Uncensored Mistral fine-tune. Creative and unrestricted outputs.",
        "url": "https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "nous-hermes-2-mistral-7b",
        "name": "Nous Hermes 2 Mistral 7B",
        "filename": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf",
        "size": "4.4 GB",
        "sizeBytes": 4600000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Nous Research's best 7B. Strong reasoning and instruction following.",
        "url": "https://huggingface.co/TheBloke/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "solar-10.7b-instruct",
        "name": "SOLAR 10.7B Instruct",
        "filename": "solar-10.7b-instruct-v1.0.Q4_K_M.gguf",
        "size": "6.1 GB",
        "sizeBytes": 6400000000,
        "ram": "~10 GB",
        "params": "10.7B",
        "description": "Upstage's merged architecture. Great general performance.",
        "url": "https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/solar-10.7b-instruct-v1.0.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },

    # ── Code Specialists ─────────────────────────────────────────────

    {
        "id": "deepseek-coder-1.3b-base",
        "name": "DeepSeek Coder 1.3B Base",
        "filename": "deepseek-coder-1.3b-base.Q4_K_M.gguf",
        "size": "0.8 GB",
        "sizeBytes": 800000000,
        "ram": "~2 GB",
        "params": "1.3B",
        "description": "Tiny coding model. Surprising performance for its size.",
        "url": "https://huggingface.co/TheBloke/deepseek-coder-1.3b-base-GGUF/resolve/main/deepseek-coder-1.3b-base.Q4_K_M.gguf",
        "tier": "tiny",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": True,
    },
    {
        "id": "deepseek-coder-6.7b-instruct",
        "name": "DeepSeek Coder 6.7B Instruct",
        "filename": "deepseek-coder-6.7b-instruct.Q4_K_M.gguf",
        "size": "4.1 GB",
        "sizeBytes": 4300000000,
        "ram": "~8 GB",
        "params": "6.7B",
        "description": "Strong coding assistant. State-of-the-art performance among 7B models.",
        "url": "https://huggingface.co/TheBloke/deepseek-coder-6.7b-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": True,
    },
    {
        "id": "deepseek-coder-v2-lite",
        "name": "DeepSeek Coder V2 Lite 16B",
        "filename": "DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf",
        "size": "5.5 GB",
        "sizeBytes": 5800000000,
        "ram": "~10 GB",
        "params": "16B (MoE)",
        "description": "Specialized coding model with mixture-of-experts. Code generation beast.",
        "url": "https://huggingface.co/bartowski/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": True,
    },
    {
        "id": "codellama-7b-instruct",
        "name": "Code Llama 7B Instruct",
        "filename": "codellama-7b-instruct.Q4_K_M.gguf",
        "size": "4.2 GB",
        "sizeBytes": 4400000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Meta's code-specialized Llama. Excellent for programming tasks.",
        "url": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": True,
    },
    {
        "id": "yi-coder-9b-chat",
        "name": "Yi-Coder 9B Chat",
        "filename": "Yi-Coder-9B-Chat-Q4_K_M.gguf",
        "size": "5.5 GB",
        "sizeBytes": 5800000000,
        "ram": "~10 GB",
        "params": "9B",
        "description": "01.AI's coding model. Strong code completion and generation.",
        "url": "https://huggingface.co/bartowski/Yi-Coder-9B-Chat-GGUF/resolve/main/Yi-Coder-9B-Chat-Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": True,
    },
    {
        "id": "wizardcoder-7b",
        "name": "WizardCoder 7B",
        "filename": "WizardCoder-Python-7B-V1.0.Q4_K_M.gguf",
        "size": "4.2 GB",
        "sizeBytes": 4400000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Microsoft's code wizard. Evolved instruction-tuned for Python.",
        "url": "https://huggingface.co/TheBloke/WizardCoder-Python-7B-V1.0-GGUF/resolve/main/WizardCoder-Python-7B-V1.0.Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": True,
    },
    {
        "id": "qwen2.5-coder-7b-instruct",
        "name": "Qwen 2.5 Coder 7B Instruct",
        "filename": "Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf",
        "size": "4.7 GB",
        "sizeBytes": 4900000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "Alibaba's dedicated coding model. Top-tier code generation.",
        "url": "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_k_m.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": True,
    },
    {
        "id": "codellama-13b-instruct",
        "name": "Code Llama 13B Instruct",
        "filename": "codellama-13b-instruct.Q4_K_M.gguf",
        "size": "7.9 GB",
        "sizeBytes": 8300000000,
        "ram": "~12 GB",
        "params": "13B",
        "description": "Larger Code Llama variant. Heavy-duty code generation.",
        "url": "https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf",
        "tier": "large",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": True,
    },

    # ── Reasoning / Thinking ─────────────────────────────────────────
    {
        "id": "deepseek-r1-distill-qwen-1.5b",
        "name": "DeepSeek R1 Distill Qwen 1.5B",
        "filename": "DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf",
        "size": "1.0 GB",
        "sizeBytes": 1050000000,
        "ram": "~2 GB",
        "params": "1.5B",
        "description": "DeepSeek R1 reasoning in a tiny package. Incredible logic for 1.5B params.",
        "url": "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf",
        "tier": "tiny",
        "supportsVision": False,
        "supportsThinking": True,
        "supportsCode": False,
    },
    {
        "id": "deepseek-r1-distill-qwen-7b",
        "name": "DeepSeek R1 Distill Qwen 7B",
        "filename": "DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf",
        "size": "4.7 GB",
        "sizeBytes": 5000000000,
        "ram": "~8 GB",
        "params": "7B",
        "description": "DeepSeek R1 reasoning distilled into Qwen 7B. Powerful logic on 8GB RAM.",
        "url": "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": True,
        "supportsCode": False,
    },
    {
        "id": "deepseek-r1-distill-qwen-32b",
        "name": "DeepSeek R1 Distill Qwen 32B",
        "filename": "DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf",
        "size": "19.3 GB",
        "sizeBytes": 19300000000,
        "ram": "~24 GB",
        "params": "32B",
        "description": "Massive reasoning model. Near top-tier performance on consumer GPUs.",
        "url": "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf",
        "tier": "large",
        "supportsVision": False,
        "supportsThinking": True,
        "supportsCode": False,
    },
    {
        "id": "deepseek-r1-distill-llama-8b",
        "name": "DeepSeek R1 Distill Llama 8B",
        "filename": "DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
        "size": "4.9 GB",
        "sizeBytes": 5100000000,
        "ram": "~8 GB",
        "params": "8B",
        "description": "DeepSeek R1 reasoning in a Llama 8B body. Think-step-by-step on a budget.",
        "url": "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
        "tier": "medium",
        "supportsVision": False,
        "supportsThinking": True,
        "supportsCode": False,
    },
    {
        "id": "deepseek-r1-distill-qwen-14b",
        "name": "DeepSeek R1 Distill Qwen 14B",
        "filename": "DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf",
        "size": "8.9 GB",
        "sizeBytes": 9300000000,
        "ram": "~14 GB",
        "params": "14B",
        "description": "DeepSeek's R1 reasoning distilled into Qwen. Chain-of-thought expert.",
        "url": "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf",
        "tier": "large",
        "supportsVision": False,
        "supportsThinking": True,
        "supportsCode": False,
    },

    # ── Large (6+ GB, needs 12–16 GB+ RAM) ──────────────────────────
    {
        "id": "qwen2.5-14b-instruct",
        "name": "Qwen 2.5 14B Instruct",
        "filename": "Qwen2.5-14B-Instruct-Q4_K_M.gguf",
        "size": "8.9 GB",
        "sizeBytes": 9300000000,
        "ram": "~14 GB",
        "params": "14B",
        "description": "Alibaba's 14B powerhouse. Near GPT-4 quality on many benchmarks.",
        "url": "https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-GGUF/resolve/main/qwen2.5-14b-instruct-q4_k_m.gguf",
        "tier": "large",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "mixtral-8x7b-instruct",
        "name": "Mixtral 8x7B Instruct",
        "filename": "Mixtral-8x7B-Instruct-v0.1.Q4_K_M.gguf",
        "size": "26 GB",
        "sizeBytes": 27000000000,
        "ram": "~32 GB",
        "params": "47B (MoE)",
        "description": "Mistral's flagship MoE. GPT-3.5 level performance, needs beefy RAM.",
        "url": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
        "tier": "large",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "wizardlm-13b",
        "name": "WizardLM 13B v1.2",
        "filename": "wizardlm-13b-v1.2.Q4_K_M.gguf",
        "size": "7.9 GB",
        "sizeBytes": 8300000000,
        "ram": "~12 GB",
        "params": "13B",
        "description": "Microsoft's instruction-evolved model. Complex task specialist.",
        "url": "https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q4_K_M.gguf",
        "tier": "large",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "nous-hermes-2-mixtral",
        "name": "Nous Hermes 2 Mixtral 8x7B",
        "filename": "Nous-Hermes-2-Mixtral-8x7B-DPO.Q4_K_M.gguf",
        "size": "26 GB",
        "sizeBytes": 27000000000,
        "ram": "~32 GB",
        "params": "47B (MoE)",
        "description": "Best open-source MoE model. Outstanding reasoning and creativity.",
        "url": "https://huggingface.co/TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mixtral-8x7B-DPO.Q4_K_M.gguf",
        "tier": "large",
        "supportsVision": False,
        "supportsThinking": False,
        "supportsCode": False,
    },

    # ── Vision / Multimodal ──────────────────────────────────────────
    {
        "id": "moondream2",
        "name": "Moondream 2 (Vision)",
        "filename": "moondream2-text-model-f16.gguf",
        "size": "2.6 GB",
        "sizeBytes": 2800000000,
        "ram": "~6 GB",
        "params": "1.9B",
        "description": "Tiny but capable vision model. Fast image analysis on low-end hardware.",
        "url": "https://huggingface.co/moondream/moondream2-gguf/resolve/main/moondream2-text-model-f16.gguf",
        "tier": "small",
        "supportsVision": True,
        "supportsThinking": False,
        "supportsCode": False,
    },
    {
        "id": "imp-v1-3b",
        "name": "Imp v1 3B (Vision)",
        "filename": "imp-v1-3b-Q4_K_M.gguf",
        "size": "1.9 GB",
        "sizeBytes": 2000000000,
        "ram": "~4 GB",
        "params": "3B",
        "description": "Lightweight multimodal model. Image captioning and visual QA on 4 GB RAM.",
        "url": "https://huggingface.co/bartowski/imp-v1-3b-GGUF/resolve/main/imp-v1-3b-Q4_K_M.gguf",
        "tier": "small",
        "supportsVision": True,
        "supportsThinking": False,
        "supportsCode": False,
    }
]


def get_catalog():
    """Return the full model catalog."""
    return MODEL_CATALOG


def find_model(model_id: str):
    """Find a model by ID in the catalog."""
    for m in MODEL_CATALOG:
        if m["id"] == model_id:
            return m
    return None
